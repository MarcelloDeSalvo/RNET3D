{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skulstripping Evaluation\n",
    "## Intro\n",
    "Given a list of nii.gz files inside the 'eval' folder, makes a prediction for the brain mask for each and saves it in the 'eval/predictions' folder unless specified otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.vedo import plot_slicer_cloud, plot_volume_cloud\n",
    "\n",
    "import utils.nifti\n",
    "importlib.reload(utils.nifti)\n",
    "from utils.nifti import estimate_volume\n",
    "\n",
    "import importlib\n",
    "import preprocessing.preprocessor\n",
    "importlib.reload(preprocessing.preprocessor)\n",
    "from preprocessing.preprocessor import MRIProcessor\n",
    "\n",
    "# neural imaging\n",
    "import nibabel as nib\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from evaluation.evaluation import *\n",
    "\n",
    "# morphology\n",
    "from skimage import morphology\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Here it's possible to change some paramaters to make predictions\n",
    "\n",
    "Paths:\n",
    "- dataset_folder (string): path to the folder that contains the files on which we want to make predictions\n",
    "- save_folder (string): path to the folder in which we want to save the predictions\n",
    "\n",
    "Save resuls:\n",
    "- save (boolean): if true, saves the predictions in save_folder\n",
    "\n",
    "3D preview:\n",
    "- show_3d_preview (boolean): shows the predicted mask in an external window\n",
    "\n",
    "Morphological smoothing:\n",
    "- remove_small_objects (boolean): if true, removes small unconnected regions based on object_min_area\n",
    "- object_min_area (int): the smallest allowable contiguos region size, in voxels\n",
    "- fill_small_holes (boolean): if true, fills small holes\n",
    "- holes_max_area (int): the maximum area, in voxels, of a contiguous hole that will be filled\n",
    "\n",
    "Prediction parameters:\n",
    "- patch_size (tuple): size of the sliding window used to extract patches from the image\n",
    "- patch_resolution (tuple): desired target resolution for all patch (should be equal to the training resolution of the model)\n",
    "- stride (int): translation offset of the sliding window (less is better but requires more computational time)\n",
    "\n",
    "Suggested stride values: 6,8,12,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../eval'\n",
    "save_folder = '../eval/predictions'\n",
    "\n",
    "structured_mode = False\n",
    "save = True\n",
    "\n",
    "show_3d_preview = False\n",
    "\n",
    "remove_small_objects = False\n",
    "object_min_area = 700000\n",
    "fill_small_holes = False\n",
    "holes_max_area = 100000\n",
    "\n",
    "patch_size = (30,30,30) #voxels\n",
    "patch_resolution = (0.2,0.2,0.2) #mm\n",
    "stride = 16\n",
    "\n",
    "pred_name = f'pred{stride}_dice_brain_mask.nii.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here the code should remain unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cropping_inference(input_volume, stride = stride, threshold = 0.5, verbose=False):\n",
    "    # Make a random volume to simulate the input\n",
    "    depth, height, width = input_volume.shape\n",
    "\n",
    "    predicted_patches = np.zeros_like(input_volume)\n",
    "    count_map = np.zeros_like(input_volume)\n",
    "\n",
    "    steps_d = max(1, (depth - patch_size[0]) // stride +1)\n",
    "    steps_h = max(1, (height - patch_size[1]) // stride +1)\n",
    "    steps_w = max(1, (width - patch_size[2]) // stride +1)\n",
    "\n",
    "    assert patch_size[0] <= depth\n",
    "    assert patch_size[1] <= height\n",
    "    assert patch_size[2] <= width\n",
    "\n",
    "    # if steps * stride < volume size, add one more step\n",
    "    if steps_d * stride < depth: steps_d += 1\n",
    "    if steps_h * stride < height: steps_h += 1\n",
    "    if steps_w * stride < width: steps_w += 1\n",
    "    # for each patch make a prediction\n",
    "    for d in range(0, steps_d * stride, stride):\n",
    "        for h in range(0, steps_h* stride, stride):\n",
    "            for w in range(0, steps_w * stride, stride):\n",
    "\n",
    "                # check if the patch is out of bounds\n",
    "                if d + patch_size[0] > depth:\n",
    "                    d = depth - patch_size[0]\n",
    "                if h + patch_size[1] > height:\n",
    "                    h = height - patch_size[1]\n",
    "                if w + patch_size[2] > width:\n",
    "                    w = width - patch_size[2]\n",
    "                \n",
    "                ranges_d = (d, d+patch_size[0])\n",
    "                ranges_h = (h, h+patch_size[1])\n",
    "                ranges_w = (w, w+patch_size[2])\n",
    "\n",
    "                # extract the current patch from the input volume\n",
    "                patch = input_volume[ranges_d[0]:ranges_d[1], ranges_h[0]:ranges_h[1], ranges_w[0]:ranges_w[1]]\n",
    "                \n",
    "                # perform inference on the patch\n",
    "                predicted_patch = 1\n",
    "\n",
    "                # sum \n",
    "                predicted_patches[ranges_d[0]:ranges_d[1], ranges_h[0]:ranges_h[1], ranges_w[0]:ranges_w[1]] += predicted_patch\n",
    "                count_map[ranges_d[0]:ranges_d[1], ranges_h[0]:ranges_h[1], ranges_w[0]:ranges_w[1]] += 1\n",
    "\n",
    "    # avoid division by 1\n",
    "    count_map[count_map==0] = 1\n",
    "    #predicted_patches /= count_map\n",
    "    #binary_predictions = (predicted_patches >= threshold).astype(np.float64)\n",
    "    \n",
    "    return predicted_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the predicted volume and the output of the random cropping inference simulation with vedo\n",
    "import vedo\n",
    "vedo.settings.default_backend= 'vtkplotter'\n",
    "from vedo import *\n",
    "from vedo.applications import Slicer3DPlotter\n",
    "\n",
    "def visualize_predictions(input_volume, pred_volume, threshold = 0.5):\n",
    "    plt = Plotter()\n",
    "\n",
    "    v2 = Volume(input_volume)\n",
    "\n",
    "    # Set voxel dimension in mm\n",
    "    v2.cmap('bone')\n",
    "\n",
    "    plt += v2\n",
    "\n",
    "    # Define class labels for each unique value in the mask\n",
    "    class_labels = np.unique(pred_volume)\n",
    "    class_labels = class_labels[class_labels != 0]\n",
    "\n",
    "    # Define a color for each unique value in the mask\n",
    "    color_map = np.random.rand(len(class_labels), 3)\n",
    "\n",
    "    # Create an empty dictionary to store point clouds\n",
    "    point_clouds = {}\n",
    "\n",
    "    i=0\n",
    "    for label in class_labels:\n",
    "        # Get voxel coordinates\n",
    "        voxel_coords = np.array(np.where(pred_volume == label)).T  * v2.spacing()\n",
    "        pts = Points(voxel_coords, r=3, c=color_map[i], alpha=0.5)\n",
    "        # Multiply by voxel dimension to get coordinates in mm\n",
    "\n",
    "        # Store the point cloud in the dictionary\n",
    "        point_clouds[label] = pts\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    for label in class_labels:\n",
    "        plt += point_clouds[label]\n",
    "\n",
    "    plt.add_slider(\n",
    "        lambda w, e: [point_clouds[label].alpha(w.value) for label in class_labels],\n",
    "        xmin=0,\n",
    "        xmax=1,\n",
    "        value=0.5,\n",
    "        pos=\"bottom-right-vertical\",\n",
    "        title=\"Opacity\",\n",
    "    )\n",
    "\n",
    "    plt.add_slider(\n",
    "        lambda w, e: v2.alpha([0, w.value]),\n",
    "        xmin=0,\n",
    "        xmax=1,\n",
    "        value=0.5,\n",
    "        pos=\"bottom-left-vertical\",\n",
    "        title=\"Opacity\",\n",
    "    )\n",
    "\n",
    "    # Make the plot rotate automatically\n",
    "    plt += Text2D(\"Press q to exit\", pos=(0.8, 0.05), s=0.8)\n",
    "\n",
    "    return plt.show(viewup='z').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_volume = np.random.rand(128,128,60)\n",
    "pred_volume = random_cropping_inference(input_volume, stride=stride, threshold=0.5)\n",
    "np.unique(pred_volume)\n",
    "visualize_predictions(input_volume, pred_volume, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
