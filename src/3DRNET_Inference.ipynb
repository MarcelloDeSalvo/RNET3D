{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-NET TBI Inference\n",
    "## Intro\n",
    "Given a list of nii.gz files inside a folder, makes a brain mask and a ROI mask prediction with a given loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from utils.utils import save_excel_table\n",
    "from utils.nifti import estimate_volume\n",
    "\n",
    "# visualization\n",
    "from utils.vedo import plot_slicer_cloud\n",
    "\n",
    "# neural imaging\n",
    "import nibabel as nib\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from evaluation.metrics import *\n",
    "\n",
    "# other\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", num_gpus)\n",
    "\n",
    "# print the version of tensorflow\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "if num_gpus>0: print(\"Cuda version: \", tf.sysconfig.get_build_info()['cuda_version'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Folder structure** <br>\n",
    "The script expects a standard folder structure similar to the followings:\n",
    "\n",
    "**STRUCTURE TYPE A:**\n",
    "```python\n",
    "C:/.../FOLDER\n",
    "  FLASH/\n",
    "    ID_MICE_A/\n",
    "      Anat/\n",
    "        ID_MICE_A.nii.gz\n",
    "        ...\n",
    "    ID_MICE_B/\n",
    "      Anat/\n",
    "        ID_MICE_B.nii.gz\n",
    "        ...\n",
    "  RARE/\n",
    "    ID_MICE_F/\n",
    "      Anat/\n",
    "        ID_MICE_F.nii.gz\n",
    "        ...\n",
    "  Other.../\n",
    "```\n",
    "\n",
    "**STRUCTURE TYPE B:**\n",
    "```python\n",
    "C:/.../FOLDER\n",
    "  FLASH/\n",
    "    ID_MICE_A.nii.gz\n",
    "    ID_MICE_B.nii.gz\n",
    "    ID_MICE_C.nii.gz\n",
    "    ...\n",
    "  Other.../\n",
    "\n",
    "```\n",
    "In this structure, there is a folder for each group of mice, such as `FLASH` or `RARE`. \n",
    "\n",
    "Inside each group folder, it is **strictly required** that each mouse file name starts with the same folder name as that of the mouse. \n",
    "\n",
    "For example, the `ID_MICE_A` folder should contain only files starting with `ID_MICE_A`. The subfolder (e.g., `Anat`) and the postfix file name extension (e.g., `N4`) can be modified later if needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**<br>\n",
    "Here it's possible to change some paramaters to make predictions. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction mode:\n",
    "- rodent_model (string): choose between 'mice' or 'rats' model\n",
    "- domain_adaptation (boolean): enables the domain adaptation to increase the number of predicted brain regions\n",
    "- default_hemisphere (string): choose between 'left' or 'right' for hemisphere division and ventricles separation if no lesion is found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodent_model = 'mice' # choose between 'mice' or 'rats' model\n",
    "domain_adaptation = False # True if you want to use the domain adaptation model, False if you want to use the original model (works only for mice)\n",
    "default_hemisfere = 'right' # right or left hemisphere division if no lesion is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths:\n",
    "- dataset_folder (List[Dataset]): list of datasets containing the MRIs of each subject\n",
    "\n",
    "```python\n",
    "# Eample\n",
    "class Dataset:\n",
    "    def __init__(self, dataset_path, subfolder='Anat', dti_mode=False):\n",
    "        self.dataset_path = dataset_path # The path to the dataset\n",
    "        self.subfolder = subfolder # The default subfolder for each subject\n",
    "        self.dti_mode = dti_mode # If the user wants to process only DTI \n",
    "\n",
    "datasets: list[Dataset] = []\n",
    "\n",
    "# Append a dataset\n",
    "datasets.append(Dataset(r'C:\\\\...\\\\data\\\\mice\\\\flash', subfolder='Anat', dti_mode=False))\n",
    "datasets.append(Dataset(r'/home/.../mice/flash', subfolder='Anat', dti_mode=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset_path, subfolder='Anat', dti_mode=False):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.subfolder = subfolder\n",
    "        self.dti_mode = dti_mode\n",
    "\n",
    "datasets: list[Dataset] = []\n",
    "\n",
    "# Add one or more datasets to the list with the append method.I:\\DannoCerebraleAcuto_MRI\\PNRR BATMAN Microbiota\\Exp GTS21\n",
    "datasets.append(Dataset(r'I:\\\\...', subfolder='', dti_mode=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results:\n",
    "- save (boolean): if true, saves the predictions inside each subject's folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D preview:\n",
    "- show_3d_preview (boolean): shows the predicted mask in an external window. \n",
    "\n",
    "Note: this blocks the execution until the window is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_3d_preview = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological smoothing:\n",
    "- remove_small_objects (boolean): if true, removes small unconnected regions based on object_min_area\n",
    "- object_min_area (int): the smallest allowable contiguos region size, in voxels\n",
    "- fill_small_holes (boolean): if true, fills small holes\n",
    "- holes_max_area (int): the maximum area, in voxels, of a contiguous hole that will be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_small_objects = True\n",
    "object_min_area = 30000\n",
    "fill_small_holes = True\n",
    "holes_max_area = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction parameters:\n",
    "- patch_size (tuple): size of the sliding window used to extract patches from the image\n",
    "- patch_resolution (tuple): desired target resolution for all patch (should be equal to the training resolution of the model)\n",
    "- stride (int): translation offset of the sliding window (less is better but requires more computational time)\n",
    "\n",
    "Suggested stride values: 6,8,12,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (80,80,80) if rodent_model == 'mice' else (96,96,96)\n",
    "patch_resolution = (0.1,0.1,0.1) #mm\n",
    "stride = 10\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and output filenames:\n",
    "- input_postfix (str): the postfix of each subject's original MRI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Image\n",
    "input_postfix = '_anat_orig'\n",
    "\n",
    "# Output Masks\n",
    "brain_prediction_postfix = '_brain_mask_r3dnet'\n",
    "roi_prediction_postfix = f'_regions_r3dnet' if not domain_adaptation else f'_regions_r3dnet_da'\n",
    "output_extension = '.nii.gz'\n",
    "\n",
    "# Excel Table Name\n",
    "excel_name = f'predicted_r3dnet_volumes.xlsx' if not domain_adaptation else f'predicted_r3dnet_da_volumes.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From here the code should remain unchanged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output labels ( Network -> Output mask labels)\n",
    "rnet_labels_mapping = {\n",
    "     0: 0,\n",
    "     1: 1,\n",
    "     2: 3,\n",
    "     3: 21\n",
    " }\n",
    "\n",
    "rnet_postprocessed_mapping = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 3,\n",
    "    3: 13,\n",
    "    4: 21\n",
    "}\n",
    "\n",
    "rnet_name_mapping = {\n",
    "    0: {'name': 'Background', 'value': 0},\n",
    "    1: {'name': 'Lesion', 'value': 1},\n",
    "    2: {'name': 'Contra Ventricle', 'value': 3},\n",
    "    3: {'name': 'Ipsi Ventricle', 'value': 13},\n",
    "    4: {'name': 'Third Ventricle', 'value': 21}\n",
    "}\n",
    "\n",
    "# For domain adaptation\n",
    "da_labels_mapping = {\n",
    "    0: 0, #0\n",
    "    1: 1, #1\n",
    "    2: 2, #2\n",
    "    3: 3, #3\n",
    "    4: 5, #5\n",
    "    5: 6, #6\n",
    "    6: 12, #12\n",
    "    7: 13, #13\n",
    "    8: 15, #15\n",
    "    9: 16, #16\n",
    "    10: 21, #21\n",
    "}\n",
    "\n",
    "da_name_mapping = {\n",
    "    0: {'name': 'Background', 'value': 0},\n",
    "    1: {'name': 'Lesion', 'value': 1},\n",
    "    2: {'name': 'Contra CC', 'value': 2},\n",
    "    3: {'name': 'Ipsi CC', 'value': 12},\n",
    "    4: {'name': 'Contra Ventricle', 'value': 3},\n",
    "    5: {'name': 'Ipsi Ventricle', 'value': 13},\n",
    "    6: {'name': 'Contra Hippo', 'value': 5},\n",
    "    7: {'name': 'Ipsi Hippo', 'value': 15},\n",
    "    8: {'name': 'Contra Cortex', 'value': 6},\n",
    "    9: {'name': 'Ipsi Cortex', 'value': 16},\n",
    "    10: {'name': 'Third Ventricle', 'value': 21},\n",
    "}\n",
    "\n",
    "da_postprocessed_mapping = da_labels_mapping\n",
    "\n",
    "\n",
    "# Selector\n",
    "if domain_adaptation:\n",
    "    postprocessed_mapping = da_postprocessed_mapping\n",
    "    name_mapping = da_name_mapping\n",
    "    labels_mapping = da_labels_mapping\n",
    "else:\n",
    "    postprocessed_mapping = rnet_postprocessed_mapping\n",
    "    name_mapping = rnet_name_mapping\n",
    "    labels_mapping = rnet_labels_mapping\n",
    "\n",
    "num_classes = len(labels_mapping)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "Load a previously trained model to start making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.networks\n",
    "from evaluation.metrics import *\n",
    "from evaluation.losses import *\n",
    "\n",
    "def get_model_path(domain_adaptation=False, rodent_model='mice'):\n",
    "    \"\"\"\n",
    "    Returns the model based on the rodent model and domain adaptation status.\n",
    "    \"\"\"\n",
    "    if rodent_model not in ['mice', 'rats']:\n",
    "        raise ValueError(\"rodent_model must be either 'mice' or 'rats'.\")\n",
    "\n",
    "    if domain_adaptation:\n",
    "        if rodent_model != 'mice':\n",
    "            raise ValueError(\"Domain adaptation is only available for the mice model.\")\n",
    "        model_name = 'rnet_da.h5'\n",
    "    else:\n",
    "        model_name = f\"rnet_{rodent_model}.h5\"\n",
    "\n",
    "    model_path = f'../models/{model_name}'\n",
    "    return model_path\n",
    "\n",
    "model_path = get_model_path(domain_adaptation=domain_adaptation, rodent_model=rodent_model)\n",
    "model = tf.keras.models.load_model(model_path,\n",
    "                                   custom_objects={ \"loss\": diceCELoss(),\n",
    "                                                    \"precision\": precision_coefficient(),\n",
    "                                                    \"sensitivity\": sensitivity_coefficient(),\n",
    "                                                    \"specificity\": specificity_coefficient(),\n",
    "                                                    \"K\": tf.keras.backend,\n",
    "                                                    \"training\": False,\n",
    "                                                  }, compile=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing configuration\n",
    "The standard pipeline order is:\n",
    "1. (opt.) Cut and pad the image to a default matrix dimension\n",
    "2. (opt.) Correct x10 intensity values\n",
    "3. Apply N4 Bias Field correction\n",
    "4. Copy the orientation from the ref. image\n",
    "5. Resample to a target resolution\n",
    "6. Normalize the intensity values with z-score (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.preprocessor import Preprocessor, Resample, Reorient, Normalize, CorrectX10, N4BiasFieldCorrection, SaveNifti\n",
    "\n",
    "# ref image for reorientation\n",
    "ref_img = nib.load(os.path.join('../example', 'RARE', 'TBI_fm_19_49', 'Anat', 'TBI_fm_19_49_N4.nii.gz'))\n",
    "\n",
    "# Create an instance of the MRIProcessor class\n",
    "processor = Preprocessor([\n",
    "    CorrectX10(),\n",
    "    N4BiasFieldCorrection(),\n",
    "    SaveNifti(postfix='_N4', replace=input_postfix),\n",
    "    Reorient(ref_img),\n",
    "    Resample(target_resolution=patch_resolution, interpolation=0),\n",
    "    Normalize()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Inference for random cropping\n",
    "Makes the predictions by sliding through the input a patch volume of size (76,76,76) with a stride of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.inference import RandomCroppingPrediction\n",
    "\n",
    "# Create an instance of the RandomCroppingPrediction class\n",
    "predictor = RandomCroppingPrediction(model, patch_size=patch_size, stride=stride, threshold=threshold, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for every subject inside the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import shutil\n",
    "from evaluation.postprocessing import ipsi_contra_division_callback, morphology_refinement_callback\n",
    "from utils.nifti import create_3d_image_from_dti\n",
    "\n",
    "def is_valid_input_file(filename, postfix):\n",
    "    return (\n",
    "        filename.startswith(postfix) or\n",
    "        filename == postfix or\n",
    "        filename.endswith(postfix + '.nii') or\n",
    "        filename.endswith(postfix + '.nii.gz')\n",
    "    )\n",
    "\n",
    "def load_and_preprocess_image(file_path, dti_mode, subject_folder, subject):\n",
    "    nii_img = nib.load(file_path)\n",
    "\n",
    "    # Handle DTI volumes\n",
    "    if dti_mode and len(nii_img.shape) > 3:\n",
    "        dti_out_name = subject + '_dti_out'\n",
    "        nii_img = create_3d_image_from_dti(nii_img, output_path=subject_folder, name=dti_out_name)\n",
    "\n",
    "    if len(nii_img.shape) > 3:\n",
    "        nii_img = nib.Nifti1Image(nii_img.get_fdata()[..., 0], affine=nii_img.affine, header=nii_img.header)\n",
    "\n",
    "    return nii_img\n",
    "\n",
    "def postprocess_and_save(x_prep, nii_img, y_mask, y_regions, subject, subject_folder, dataset):\n",
    "    # Apply post-processing\n",
    "    y_mask = morphology_refinement_callback(\n",
    "        fill_small_holes=fill_small_holes,\n",
    "        holes_max_area=holes_max_area,\n",
    "        remove_small_objects=remove_small_objects,\n",
    "        object_min_area=object_min_area\n",
    "    )(y_mask)\n",
    "\n",
    "    if not domain_adaptation:\n",
    "        y_regions = ipsi_contra_division_callback(\n",
    "            use_centroids=True,\n",
    "            default_hemisphere=default_hemisfere\n",
    "        )(y_regions)\n",
    "\n",
    "    # Wrap in NIfTI\n",
    "    y_pred_nifti = nib.Nifti1Image(y_regions, affine=x_prep.affine, dtype=np.float64, header=x_prep.header)\n",
    "    y_pred_mask_nifti = nib.Nifti1Image(y_mask, affine=x_prep.affine, dtype=np.float64, header=x_prep.header)\n",
    "\n",
    "    if show_3d_preview:\n",
    "        plot_slicer_cloud(x_prep, y_pred_nifti)\n",
    "\n",
    "    # Save outputs\n",
    "    roi_save_path = os.path.join(subject_folder, subject + roi_prediction_postfix + output_extension)\n",
    "    mask_save_path = os.path.join(subject_folder, subject + brain_prediction_postfix + output_extension)\n",
    "\n",
    "    if not dataset.dti_mode:\n",
    "        final_image = processor.deprocess(y_pred_nifti, nii_img, postprocessed_mapping, save_path=roi_save_path, verbose=False)\n",
    "        estimate_volume(final_image, resolution=patch_resolution, verbose=True)\n",
    "\n",
    "    processor.deprocess(y_pred_mask_nifti, nii_img, postprocessed_mapping, save_path=mask_save_path, verbose=False)\n",
    "\n",
    "def process_subject(subject_folder, subject, dataset):\n",
    "    files = [\n",
    "        f for f in os.listdir(subject_folder)\n",
    "        if is_valid_input_file(f, input_postfix)\n",
    "    ]\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"File with postfix {input_postfix} not found for {subject}, skipping\")\n",
    "        return\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            print('\\n|-', file, '-------------------\\ \\n')\n",
    "            file_path = os.path.join(subject_folder, file)\n",
    "\n",
    "            nii_img = load_and_preprocess_image(file_path, dataset.dti_mode, subject_folder, subject)\n",
    "            x_prep = processor.preprocess(nii_img, path=file_path)\n",
    "\n",
    "            results = predictor.random_cropping_inference(x_prep, with_brain_mask=True)\n",
    "            postprocess_and_save(x_prep, nii_img, results['brain_mask'], results['roi'], subject, subject_folder, dataset)\n",
    "\n",
    "            print('----------------------------------------------------------// \\n\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing case {subject}: {e}\")\n",
    "\n",
    "def run_prediction_pipeline(datasets):\n",
    "    for dataset in datasets:\n",
    "        base_path = dataset.dataset_path\n",
    "        if not os.path.isdir(base_path):\n",
    "            print(f'Folder {base_path} not found, skipping')\n",
    "            continue\n",
    "\n",
    "        print(f'\\n| Processing dataset: {dataset.dataset_path} --')\n",
    "\n",
    "        for subject in os.listdir(base_path):\n",
    "            subject_path = os.path.join(base_path, subject)\n",
    "\n",
    "            # Check if it's a file (not a folder)\n",
    "            if not os.path.isdir(subject_path):\n",
    "                # Remove .nii, .nii.gz, or .gz extension to create a folder name\n",
    "                if subject.endswith('.nii.gz'):\n",
    "                    subject_name = subject[:-7]\n",
    "                elif subject.endswith('.nii') or subject.endswith('.gz'):\n",
    "                    subject_name = os.path.splitext(subject)[0]\n",
    "                else:\n",
    "                    subject_name = os.path.splitext(subject)[0]\n",
    "\n",
    "                # Remove input_postfix if present\n",
    "                if input_postfix and subject_name.endswith(input_postfix):\n",
    "                    subject_name = subject_name[: -len(input_postfix)]\n",
    "\n",
    "\n",
    "                subject_folder = os.path.join(base_path, subject_name, dataset.subfolder)\n",
    "                os.makedirs(subject_folder, exist_ok=True)\n",
    "\n",
    "                # Move file into the new subject folder\n",
    "                shutil.copy(subject_path, os.path.join(subject_folder, subject))\n",
    "            else:\n",
    "                subject_folder = os.path.join(subject_path, dataset.subfolder)\n",
    "                subject_name = subject\n",
    "\n",
    "            process_subject(subject_folder, subject_name, dataset)\n",
    "            \n",
    "        # Save volume stats\n",
    "        save_excel_table(\n",
    "            base_path,\n",
    "            sub_folder=dataset.subfolder,\n",
    "            include_only_list=None,\n",
    "            save_folder=base_path,\n",
    "            pred_roi_name=roi_prediction_postfix + output_extension,\n",
    "            pred_brain_name=brain_prediction_postfix + output_extension,\n",
    "            name_mapping=name_mapping,\n",
    "            file_name=excel_name,\n",
    "            postfix_mode=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prediction pipeline\n",
    "run_prediction_pipeline(datasets=datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
